{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import noisereduce as nr\n",
    "import webrtcvad\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def reduce_noise(audio_data):\n",
    "    stft = librosa.stft(audio_data)\n",
    "    stft_db = librosa.amplitude_to_db(np.abs(stft))\n",
    "    mean_noise = np.mean(stft_db[:, :200], axis=1)\n",
    "    stft_db -= mean_noise[:, np.newaxis]\n",
    "    noise_reduced_audio = librosa.griffinlim(librosa.db_to_amplitude(stft_db))\n",
    "    return noise_reduced_audio\n",
    "\n",
    "def vad_energy_based(audio_data, threshold=0.05):\n",
    "    active_segments = []  # Initialize an empty list to store active voice segments\n",
    "    segment_start = 0  # Variable to store the start index of an active segment\n",
    "    is_previous_active = False  # Flag to keep track of the previous state of voice activity\n",
    "\n",
    "    for i, sample in enumerate(audio_data):\n",
    "        energy = np.sum(sample ** 2)  # Calculate energy of the current sample\n",
    "        is_active = energy > threshold  # Check if the current sample indicates voice activity\n",
    "\n",
    "        if is_active:\n",
    "            if not is_previous_active:\n",
    "                segment_start = i  # Start of a new active segment\n",
    "            is_previous_active = True\n",
    "        else:\n",
    "            if is_previous_active:\n",
    "                active_segments.append(audio_data[segment_start:i])  # Append active segment to list\n",
    "            is_previous_active = False\n",
    "\n",
    "    # Check if the last segment is active\n",
    "    if is_previous_active:\n",
    "        active_segments.append(audio_data[segment_start:])\n",
    "\n",
    "    # Concatenate all active segments into a single numpy array\n",
    "    if active_segments:\n",
    "        active_voice_array = np.concatenate(active_segments)\n",
    "        return active_voice_array\n",
    "    else:\n",
    "        return np.array([])  # Return an empty array if no active segments are found\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2980160,)\n"
     ]
    }
   ],
   "source": [
    "# Load the WAV audio file\n",
    "audio_file = 'output_file.wav'\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "def generateOutputAudioFile(outputDestination, inputArray):\n",
    "\n",
    "    # Scale the values in the array to the range [-32768, 32767] (for 16-bit PCM audio)\n",
    "    scaled_array = np.int16(inputArray*32767)\n",
    "\n",
    "    # Write the array to a WAV file\n",
    "    write(outputDestination, sr, scaled_array)\n",
    "\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "(2980160,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Resample the audio to a common sampling rate (e.g., 16 kHz) - Preprocessing 1 Desampling\n",
    "target_sr = 16000\n",
    "print(sr)\n",
    "y_resampled = librosa.resample(y, orig_sr=sr, target_sr=16000)\n",
    "print(y_resampled.shape)\n",
    "\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_1_output.wav', y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the audio to ensure consistent amplitude levels - Preprocessing 2 Normalization\n",
    "y_normalized = librosa.util.normalize(y_resampled)\n",
    "\n",
    "print(y_normalized.shape)\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_2_output.wav', y_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove silence using a threshold (e.g., -40 dB) - Preprocessing 3 Silence Removal\n",
    "y_trimmed, _ = librosa.effects.trim(y_normalized, top_db=3)\n",
    "\n",
    "print(y_trimmed.shape)\n",
    "\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_3_output.wav', y_trimmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2785280,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform noise reduction using the NoiseReduce library - Preprocessing 4 Noise Reduction\n",
    "y_denoised = reduce_noise(y_trimmed) \n",
    "print(y_denoised.shape)\n",
    "\n",
    "\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_4_output.wav', y_denoised)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Voice Array: (1663428,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# speech_segments = perform_vad(y_denoised, sr) - Preprocessing 5 Voice Activity Detection (VAD):\n",
    "active_voice_array = vad_energy_based(y_denoised)\n",
    "print(\"Active Voice Array:\", active_voice_array.shape)\n",
    "y_preprocessed5 = active_voice_array\n",
    "# y_preprocessed5 = np.concatenate(speech_segments)\n",
    "print(y_preprocessed5.shape)\n",
    "\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_5_output.wav', active_voice_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1663428,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find peaks in the audio signal - Preprocessing 6  Dynamic Range Compression:\n",
    "peaks, _ = find_peaks(np.abs(y_preprocessed5), height=0.5)\n",
    "\n",
    "# Apply compression by reducing the amplitude of peaks\n",
    "compression_factor = 0.5\n",
    "y_compressed = np.copy(y_preprocessed5)\n",
    "\n",
    "y_compressed[peaks] *= compression_factor\n",
    "print(y_compressed.shape)\n",
    "# Now y_compressed contains the audio with dynamic range compression applied\n",
    "\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_6_output.wav', y_compressed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1663428, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize StandardScaler for feature scaling - Preprocessing 7\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape y_compressed to a 2D array (assuming it's a 1D array representing audio signal)\n",
    "y_reshaped = y_compressed.reshape(-1, 1)\n",
    "\n",
    "# Scale the features (audio samples) using the scaler\n",
    "scaled_audio = scaler.fit_transform(y_reshaped)\n",
    "\n",
    "# Reshape scaled_audio back to 1D array (if needed)\n",
    "scaled_audio = scaled_audio.ravel()\n",
    "\n",
    "# Now scaled_audio contains the scaled audio signal with zero mean and unit variance\n",
    "print(y_reshaped.shape)\n",
    "\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_7_output.wav', scaled_audio)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
