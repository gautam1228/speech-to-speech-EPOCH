{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import noisereduce as nr\n",
    "import webrtcvad\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.io.wavfile import write\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch, Shift, ClippingDistortion, Gain\n",
    "import numpy as np\n",
    "import audiomentations \n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def normalize_audio(audio_segment, target_dBFS=-20.0):\n",
    "    # Calculate the current dBFS (decibels relative to full scale) of the audio\n",
    "    current_dBFS = audio_segment.dBFS\n",
    "    \n",
    "    # Calculate the dBFS difference needed to reach the target level\n",
    "    dBFS_difference = target_dBFS - current_dBFS\n",
    "    \n",
    "    # Apply the gain adjustment to normalize the audio to the target level\n",
    "    normalized_audio = audio_segment.apply_gain(dBFS_difference)\n",
    "    \n",
    "    return normalized_audio\n",
    "\n",
    "\n",
    "def augment_audio(audio_signal, sr):\n",
    "    \"\"\"\n",
    "    Augment the audio signal with various transformations.\n",
    "    \n",
    "    Args:\n",
    "    - audio_signal (numpy.ndarray): 1D array representing the audio signal.\n",
    "    - sr (int): Sampling rate of the audio signal.\n",
    "    \n",
    "    Returns:\n",
    "    - augmented_signal (numpy.ndarray): Augmented audio signal.\n",
    "    \"\"\"\n",
    "    # Define augmentation pipeline\n",
    "\n",
    "    augmentation_pipeline = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.3),  # Reduced probability to 0.3\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=0.3),  # Reduced probability to 0.3\n",
    "        TimeStretch(min_rate=0.8, max_rate=1.2, p=0.3),  # Reduced probability to 0.3\n",
    "        ClippingDistortion(max_percentile_threshold=95, p=0.3),  # Reduced probability to 0.3\n",
    "        Gain(min_gain_in_db=-6, max_gain_in_db=6, p=0.3)  # Reduced probability to 0.3\n",
    "])\n",
    "\n",
    "    \n",
    "    # Augment the audio signal\n",
    "    augmented_signal = augmentation_pipeline(samples=audio_signal, sample_rate=sr)\n",
    "    \n",
    "    return augmented_signal\n",
    "\n",
    "def remove_clipping_artifacts(audio_signal, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Remove clipping artifacts from the audio signal.\n",
    "    \n",
    "    Args:\n",
    "    - audio_signal (numpy.ndarray): 1D array representing the audio signal.\n",
    "    - threshold (float): Threshold for detecting clipping artifacts (default: 0.95).\n",
    "    \n",
    "    Returns:\n",
    "    - processed_signal (numpy.ndarray): Processed audio signal with clipping artifacts removed.\n",
    "    \"\"\"\n",
    "    # Find the maximum absolute value in the audio signal\n",
    "    max_abs_value = np.max(np.abs(audio_signal))\n",
    "    \n",
    "    # Detect clipping artifacts based on the threshold\n",
    "    clipping_mask = np.abs(audio_signal) >= threshold * max_abs_value\n",
    "    \n",
    "    # Replace clipped samples with interpolated values\n",
    "    processed_signal = np.copy(audio_signal)\n",
    "    processed_signal[clipping_mask] = np.nan  # Mark clipped samples as NaN\n",
    "    processed_signal = np.nan_to_num(processed_signal)  # Replace NaN with interpolated values\n",
    "    \n",
    "    return processed_signal\n",
    "\n",
    "\n",
    "def reduce_noise(audio_data, sample_rate, stationaryTF):\n",
    "\n",
    "    # Apply spectral subtraction to reduce noise\n",
    "    chunk_length = 30000  # 30 seconds\n",
    "    reduced_noise = []\n",
    "    for i in range(0, len(audio_data), chunk_length):\n",
    "        chunk = audio_data[i:i+chunk_length]\n",
    "        reduced_chunk = nr.reduce_noise(y=chunk, sr=sample_rate, stationary = stationaryTF)\n",
    "        reduced_noise.append(reduced_chunk)\n",
    "\n",
    "    reduced_noise = np.concatenate(reduced_noise)\n",
    "    return reduced_noise\n",
    "\n",
    "def vad_energy_based(audio_data, threshold=0.00003):\n",
    "    active_segments = []  # Initialize an empty list to store active voice segments\n",
    "    segment_start = 0  # Variable to store the start index of an active segment\n",
    "    is_previous_active = False  # Flag to keep track of the previous state of voice activity\n",
    "\n",
    "    for i, sample in enumerate(audio_data):\n",
    "        energy = np.sum(sample ** 2)  # Calculate energy of the current sample\n",
    "        is_active = energy > threshold  # Check if the current sample indicates voice activity\n",
    "\n",
    "        if is_active:\n",
    "            if not is_previous_active:\n",
    "                segment_start = i  # Start of a new active segment\n",
    "            is_previous_active = True\n",
    "        else:\n",
    "            if is_previous_active:\n",
    "                active_segments.append(audio_data[segment_start:i])  # Append active segment to list\n",
    "            is_previous_active = False\n",
    "\n",
    "    # Check if the last segment is active\n",
    "    if is_previous_active:\n",
    "        active_segments.append(audio_data[segment_start:])\n",
    "\n",
    "    # Concatenate all active segments into a single numpy array\n",
    "    if active_segments:\n",
    "        active_voice_array = np.concatenate(active_segments)\n",
    "        return active_voice_array\n",
    "    else:\n",
    "        return np.array([])  # Return an empty array if no active segments are found\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2980160,)\n"
     ]
    }
   ],
   "source": [
    "# Load the WAV audio file\n",
    "audio_file = 'output_file.wav'\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "def generateOutputAudioFile(outputDestination, inputArray):\n",
    "\n",
    "    # Scale the values in the array to the range [-32768, 32767] (for 16-bit PCM audio)\n",
    "    scaled_array = np.int16(inputArray*32767)\n",
    "\n",
    "    # Write the array to a WAV file\n",
    "    write(outputDestination, sr, scaled_array)\n",
    "\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "(2980160,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Resample the audio to a common sampling rate (e.g., 16 kHz) - Preprocessing 1 Desampling\n",
    "target_sr = 16000\n",
    "print(sr)\n",
    "y_resampled = librosa.resample(y, orig_sr=sr, target_sr=16000)\n",
    "print(y_resampled.shape)\n",
    "\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_1_output.wav', y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2980160,)\n"
     ]
    }
   ],
   "source": [
    "# Perform noise reduction using the NoiseReduce library - Preprocessing 4 Noise Reduction\n",
    "y_denoised = reduce_noise( y_resampled, sr, True) \n",
    "\n",
    "print(y_denoised.shape)\n",
    "\n",
    "factor = 10**(10 / 20)\n",
    "    \n",
    "    # Multiply each element of the array by the factor\n",
    "y_denoised = y_denoised * factor\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_3_output.wav', y_denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2980160,)\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_models\\\\2stems', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.7\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from pretrained_models\\2stems\\model\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(2980160,)\n"
     ]
    }
   ],
   "source": [
    "from spleeter.separator import Separator\n",
    "\n",
    "# Load Spleeter separator\n",
    "separator = Separator('spleeter:2stems')  # 'spleeter:2stems' separates into vocal and accompaniment\n",
    "y_resampled = y_denoised\n",
    "print(y_resampled.shape)\n",
    "y_resampled = y_resampled.reshape(-1, 1)\n",
    "# Separate audio sources\n",
    "separated_sources = separator.separate( y_resampled)\n",
    "y_musicremoved = separated_sources['vocals']\n",
    "\n",
    "y_musicremoved = y_musicremoved[:, 0]\n",
    "print(y_musicremoved.shape)\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_2_output.wav', y_musicremoved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform noise reduction using the NoiseReduce library - Preprocessing 4 Noise Reduction\n",
    "\n",
    "# print(y_denoised.shape)\n",
    "\n",
    "# factor = 10**(10 / 20)\n",
    "    \n",
    "    # Multiply each element of the array by the factor\n",
    "# y_musicremoved = y_musicremoved * factor\n",
    "# y_denoised = reduce_noise( y_musicremoved, sr, True) \n",
    "# generateOutputAudioFile('pre_processed_audio/pre_processing_3_output.wav', y_denoised)\n",
    "# from pydub import AudioSegment\n",
    "# from pydub.silence import split_on_silence\n",
    "# # Split audio into segments based on silence\n",
    "# audio = AudioSegment.from_wav('pre_processed_audio/pre_processing_4_output.wav')\n",
    "# segments = split_on_silence(audio, min_silence_len=100, silence_thresh=-90)\n",
    "\n",
    "# # Modify video segments\n",
    "# modified_video_clips = []\n",
    "# # Trim segments to a fixed duration (e.g., 5 seconds)\n",
    "# fixed_duration = 5000  # 5 seconds in milliseconds\n",
    "\n",
    "# # Create new segments with fixed duration\n",
    "# trimmed_segments = [segment[:fixed_duration] for segment in segments]\n",
    "\n",
    "# # Export trimmed segments\n",
    "# concatenated_audio = trimmed_segments[0]\n",
    "# for segment in trimmed_segments[1:]:\n",
    "#     concatenated_audio += segment\n",
    "\n",
    "# # Export concatenated audio\n",
    "# concatenated_audio.export(\"concatenated_audio.wav\", format=\"wav\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2754560,)\n"
     ]
    }
   ],
   "source": [
    "# Remove silence using a threshold (e.g., -40 dB) - Preprocessing 3 Silence Removal\n",
    "y_trimmed, _ = librosa.effects.trim(y_musicremoved, top_db=8)\n",
    "print(y_trimmed.shape)\n",
    "\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_6_output.wav', y_trimmed)\n",
    "\n",
    "# Normalize the audio to ensure consistent amplitude levels - Preprocessing 2 Normalization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2754560,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_normalized = librosa.util.normalize(y_trimmed, norm=np.inf)\n",
    "\n",
    "print(y_normalized.shape)\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_5_output.wav', y_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # speech_segments = perform_vad(y_denoised, sr) - Preprocessing 5 Voice Activity Detection (VAD):\n",
    "# active_voice_array = vad_energy_based(y_denoised)\n",
    "# print(\"Active Voice Array:\", active_voice_array.shape)\n",
    "# y_preprocessed5 = active_voice_array\n",
    "# # y_preprocessed5 = np.concatenate(speech_segments)\n",
    "y_preprocessed5 = y_normalized\n",
    "# print(y_preprocessed5.shape)\n",
    "\n",
    "# generateOutputAudioFile('pre_processed_audio/pre_processing_5_output.wav', y_preprocessed5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2754560,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find peaks in the audio signal - Preprocessing 6  Dynamic Range Compression:\n",
    "peaks, _ = find_peaks(np.abs(y_preprocessed5), height=1)\n",
    "\n",
    "# Apply compression by reducing the amplitude of peaks\n",
    "compression_factor = 0.5\n",
    "y_compressed = np.copy(y_preprocessed5)\n",
    "\n",
    "y_compressed[peaks] *= compression_factor\n",
    "print(y_compressed.shape)\n",
    "# Now y_compressed contains the audio with dynamic range compression applied\n",
    "\n",
    "generateOutputAudioFile('pre_processed_audio/pre_processing_7_output.wav', y_compressed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Attention, Dropout\n",
    "\n",
    "class Tacotron(tf.keras.Model):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, encoder_units, decoder_units, attention_units, dropout_rate):\n",
    "        super(Tacotron, self).__init__()\n",
    "        self.encoder = Encoder(num_encoder_layers, encoder_units, dropout_rate)\n",
    "        self.decoder = Decoder(num_decoder_layers, decoder_units, attention_units, dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        source_audio, target_audio = inputs\n",
    "        encoder_output = self.encoder(source_audio, training=training)\n",
    "        decoder_output = self.decoder([encoder_output, target_audio], training=training)\n",
    "        return decoder_output\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, units, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.conv_layers = [Conv1D(units, kernel_size=5, strides=1, padding='same', activation='relu') \n",
    "                            for _ in range(num_layers)]\n",
    "        self.dropout_layers = [Dropout(dropout_rate) for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.conv_layers[i](x)\n",
    "            x = self.dropout_layers[i](x, training=training)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, units, attention_units, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.units = units\n",
    "        self.attention = Attention()\n",
    "        self.lstm_layers = [LSTM(units, return_sequences=True, return_state=True) \n",
    "                            for _ in range(num_layers)]\n",
    "        self.fc = Dense(units)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        encoder_output, target_audio = inputs\n",
    "        context_vector, attention_weights = self.attention([encoder_output, target_audio])\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), target_audio], axis=-1)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x, _, _ = self.lstm_layers[i](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Define hyperparameters\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 2\n",
    "encoder_units = 256\n",
    "decoder_units = 512\n",
    "attention_units = 256\n",
    "dropout_rate = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "# Load Tacotron model\n",
    "model = Tacotron(num_encoder_layers, num_decoder_layers, encoder_units, decoder_units, attention_units, dropout_rate)\n",
    "model.load_weights('model_new.pth') #Works upon training of the model, takes some time\n",
    "# Assuming y_compressed is a numpy array containing audio data\n",
    "y_compressed = np.random.randn(1, 10, 128)  # Example data, replace with your data\n",
    "\n",
    "# Generate audio from numpy array\n",
    "generated_audio = model([y_compressed, y_compressed], training=False)\n",
    "\n",
    "# Convert generated audio tensor to numpy array\n",
    "generated_audio_np = generated_audio.numpy()\n",
    "\n",
    "# Define audio parameters\n",
    "sampling_rate = 16000  # Update with your desired sampling rate\n",
    "output_file = \"generated_audio.wav\"\n",
    "\n",
    "# Save generated audio as WAV file\n",
    "sf.write(output_file, generated_audio_np[0], sampling_rate)\n",
    "\n",
    "print(\"Generated audio saved as\", output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
